import streamlit as st
import cv2
import mediapipe as mp
import joblib
import numpy as np
import time
import os

def show():
    main_title_cfg = """
            <div style="display: flex; justify-content: center; align-items: center; padding: 0; margin: 0;">
                <h1 style="
                    color: #ff40b5;
                    background: white;
                    padding: 15px 30px;
                    border-radius: 15px;
                    font-size: 36px;
                    font-family: 'Segoe UI', 'Archivo', sans-serif;
                    box-shadow: 0 4px 15px rgba(0, 0, 0, 0.1);
                    text-align: center;
                    margin-top: 10px;
                    margin-bottom: 0;
                ">
                    üòê <b>Nh·∫≠n di·ªán Bi·ªÉu C·∫£m Khu√¥n M·∫∑t (Emotion Detection)</b>
                </h1>
            </div>
            """

    st.markdown(main_title_cfg, unsafe_allow_html=True)

    run = st.checkbox('B·∫Øt ƒë·∫ßu nh·∫≠n di·ªán bi·ªÉu c·∫£m', key='emotion_run_checkbox')

    video_source_emotion = st.selectbox(
        "Ch·ªçn ngu·ªìn video cho bi·ªÉu c·∫£m",
        ("webcam", "video"),
        key="emotion_video_source_selectbox"
    )

    video_file_emotion = None
    if video_source_emotion == "video":
        video_file_emotion = st.file_uploader("Ch·ªçn video cho bi·ªÉu c·∫£m", type=["mp4", "avi", "mov", "mkv"], key="emotion_video_uploader")

    FRAME_WINDOW_EMOTION = st.image([])

    cap = None

    if run:
        if video_source_emotion == "webcam":
            cap = cv2.VideoCapture(0)
            if not cap.isOpened():
                st.error("Kh√¥ng th·ªÉ m·ªü webcam. Vui l√≤ng ki·ªÉm tra camera v√† quy·ªÅn truy c·∫≠p.")
                st.stop()
        elif video_source_emotion == "video" and video_file_emotion is not None:
            temp_video_file_path = os.path.join("temp_emotion_video.mp4")
            with open(temp_video_file_path, "wb") as f:
                f.write(video_file_emotion.read())
            cap = cv2.VideoCapture(temp_video_file_path)
            if not cap.isOpened():
                st.error(f"Kh√¥ng th·ªÉ m·ªü file video: {video_file_emotion.name}")
                st.stop()
        elif video_source_emotion == "video" and video_file_emotion is None:
            st.warning("Vui l√≤ng t·∫£i l√™n m·ªôt file video ƒë·ªÉ ti·∫øp t·ª•c.")
            st.stop()
        else: 
            st.stop()

        if cap is not None and cap.isOpened():
            st.info("ƒêang x·ª≠ l√Ω video/webcam...")

            # Kh·ªüi t·∫°o Mediapipe
            mp_face_mesh = mp.solutions.face_mesh
            mp_drawing = mp.solutions.drawing_utils
            mp_drawing_styles = mp.solutions.drawing_styles

            # T·∫£i m√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán v√† scaler
            model_path = 'NhanDienBieuCamKhuonMat/face_expression_model.joblib' 
            try:
                model, scaler = joblib.load(model_path)
            except FileNotFoundError:
                st.error(f"L·ªói: Kh√¥ng t√¨m th·∫•y file model '{model_path}'.")
                st.error("H√£y ƒë·∫£m b·∫£o b·∫°n ƒë√£ hu·∫•n luy·ªán v√† l∆∞u m√¥ h√¨nh v√†o ƒë√∫ng ƒë∆∞·ªùng d·∫´n.")
                st.stop()
            except Exception as e:
                st.error(f"L·ªói khi t·∫£i model: {e}")
                st.stop()

            # L·∫•y danh s√°ch c√°c l·ªõp (bi·ªÉu c·∫£m) m√† m√¥ h√¨nh ƒë√£ h·ªçc
            try:
                class_labels = model.classes_
                st.caption(f"M√¥ h√¨nh ƒë∆∞·ª£c hu·∫•n luy·ªán ƒë·ªÉ nh·∫≠n di·ªán c√°c bi·ªÉu c·∫£m: {class_labels}")
            except AttributeError:
                st.warning("Kh√¥ng th·ªÉ l·∫•y `model.classes_`. ƒê·∫£m b·∫£o m√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c hu·∫•n luy·ªán ƒë√∫ng c√°ch.")
                class_labels = None 

            # Bi·∫øn ƒë·ªÉ ki·ªÉm so√°t t·∫ßn su·∫•t d·ª± ƒëo√°n (gi·∫£m t·∫£i CPU)
            last_prediction_time = time.time()
            prediction_interval = 0.3 # Gi√¢y, d·ª± ƒëo√°n m·ªói 0.3 gi√¢y
            current_expression_display = "Dang phan tich..."

            # S·ªë l∆∞·ª£ng landmarks k·ª≥ v·ªçng (478 khi refine_landmarks=True)
            EXPECTED_LANDMARKS_COUNT = 478 
            EXPECTED_FEATURES_COUNT = EXPECTED_LANDMARKS_COUNT * 3

            with mp_face_mesh.FaceMesh(
                max_num_faces=1,
                refine_landmarks=True, 
                min_detection_confidence=0.5,
                min_tracking_confidence=0.5) as face_mesh:

                while run and cap.isOpened():
                    success, image = cap.read()
                    if not success:
                        if video_source_emotion == "video":
                            st.write("ƒê√£ h·∫øt video. ƒê·ªÉ xem l·∫°i, b·ªè ch·ªçn 'B·∫Øt ƒë·∫ßu' r·ªìi ch·ªçn l·∫°i.")
                            break 
                        else:
                            st.warning("Kh√¥ng th·ªÉ ƒë·ªçc frame t·ª´ webcam.")
                            break

                    image = cv2.flip(image, 1) # L·∫≠t ·∫£nh ƒë·ªÉ gi·ªëng nh∆∞ g∆∞∆°ng
                    
                    # ƒê·ªÉ c·∫£i thi·ªán hi·ªáu su·∫•t, t√πy ch·ªçn ƒë√°nh d·∫•u h√¨nh ·∫£nh l√† kh√¥ng th·ªÉ ghi ƒë√®.
                    image_for_processing = image.copy() 
                    image_for_processing.flags.writeable = False
                    image_rgb = cv2.cvtColor(image_for_processing, cv2.COLOR_BGR2RGB)
                    results = face_mesh.process(image_rgb)
                    
                    image_for_processing.flags.writeable = True 
                    image_bgr_output = image

                    if results.multi_face_landmarks:
                        face_landmarks = results.multi_face_landmarks[0] # L·∫•y khu√¥n m·∫∑t ƒë·∫ßu ti√™n

                        # Draw face mesh
                        mp_drawing.draw_landmarks(
                            image=image_bgr_output,
                            landmark_list=face_landmarks,
                            connections=mp_face_mesh.FACEMESH_TESSELATION,
                            landmark_drawing_spec=None,
                            connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_tesselation_style())
                        mp_drawing.draw_landmarks(
                            image=image_bgr_output,
                            landmark_list=face_landmarks,
                            connections=mp_face_mesh.FACEMESH_CONTOURS,
                            landmark_drawing_spec=None,
                            connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_contours_style())

                        # Ch·ªâ th·ª±c hi·ªán d·ª± ƒëo√°n sau m·ªôt kho·∫£ng th·ªùi gian nh·∫•t ƒë·ªãnh
                        current_time = time.time()
                        if current_time - last_prediction_time > prediction_interval:
                            landmarks_data = []
                            for landmark_point in face_landmarks.landmark:
                                landmarks_data.extend([landmark_point.x, landmark_point.y, landmark_point.z])

                            if len(landmarks_data) == EXPECTED_FEATURES_COUNT:
                                scaled_landmarks = scaler.transform(np.array(landmarks_data).reshape(1, -1))
                                
                                # D·ª± ƒëo√°n bi·ªÉu c·∫£m
                                prediction_proba = model.predict_proba(scaled_landmarks)
                                prediction_index = np.argmax(prediction_proba)
                                
                                if class_labels is not None and prediction_index < len(class_labels):
                                    predicted_expression_name = class_labels[prediction_index]
                                    confidence = prediction_proba[0][prediction_index]
                                    current_expression_display = f"{predicted_expression_name} ({confidence:.2f})"
                                else:
                                    current_expression_display = f"Bieu cam: ID {prediction_index} ({np.max(prediction_proba):.2f})"
                                
                                last_prediction_time = current_time
                            else:
                                current_expression_display = f"Landmarks: {len(landmarks_data)}/{EXPECTED_FEATURES_COUNT}"
                    else:
                        current_expression_display = "Khong tim thay khuon mat"

                    # Hi·ªÉn th·ªã bi·ªÉu c·∫£m d·ª± ƒëo√°n l√™n ·∫£nh
                    cv2.putText(image_bgr_output, current_expression_display, (10, 30), 
                                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2, cv2.LINE_AA)

                    # Chuy·ªÉn BGR -> RGB ƒë·ªÉ hi·ªÉn th·ªã trong Streamlit
                    frame_to_display = cv2.cvtColor(image_bgr_output, cv2.COLOR_BGR2RGB)
                    FRAME_WINDOW_EMOTION.image(frame_to_display, channels="RGB")
                    
                    time.sleep(0.01) 

                # End of while loop
                if video_source_emotion == "video" and video_file_emotion is not None:
                    st.write("K·∫øt th√∫c x·ª≠ l√Ω video.")
                elif video_source_emotion == "webcam":
                     st.write("ƒê√£ d·ª´ng webcam.")

            # Cleanup
            if cap is not None:
                cap.release()
            if video_source_emotion == "video" and video_file_emotion is not None:
                if os.path.exists(temp_video_file_path):
                    try:
                        os.remove(temp_video_file_path) # Clean up temp file
                    except Exception as e:
                        st.warning(f"Kh√¥ng th·ªÉ x√≥a file t·∫°m: {e}")
            
            if not run:
                FRAME_WINDOW_EMOTION.empty()
                st.info("ƒê√£ d·ª´ng nh·∫≠n di·ªán bi·ªÉu c·∫£m.")


    elif not run: 
        FRAME_WINDOW_EMOTION.empty()
        if cap is not None:
            cap.release()
        st.info("Nh·∫•n 'B·∫Øt ƒë·∫ßu nh·∫≠n di·ªán bi·ªÉu c·∫£m' ƒë·ªÉ ch·∫°y.")

if __name__ == '__main__':
    st.title("Test Nh·∫≠n Di·ªán Bi·ªÉu C·∫£m Khu√¥n M·∫∑t")
    show()
